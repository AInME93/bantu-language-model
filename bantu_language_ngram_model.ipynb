{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## N-Gram\n",
        "\n",
        "For this solution we use the N-Gram model approach. The steps will be as follows:\n",
        "\n",
        "1. Ingest data\n",
        "2. Generate vocab, encoder, decoder\n",
        "3. Generate a count dictionary of all n-grams generated from our data. This will serve as our frequency lookup table.\n",
        "4. Read test data.\n",
        "5. Iteratively for each character:\n",
        "\n",
        "  * generate a bigram matching the length specified earlier. Where necessary, using padding character to maintain length.\n",
        "\n",
        "  * Use our frequency lookup table to return count of the current bigram \n",
        "\n",
        "  * Divide by total no of occurences of all bigrams \n",
        "\n",
        "  * Normalize these probabilities and return the probability matrix generated\n",
        "\n",
        "  * Look up generated probability of our expected character\n",
        "\n",
        "  * calculate log of the value and subtract from overall loss of the entire model\n",
        "\n",
        "  * remove first character of our bigram and append current character to the end of the bigram\n",
        "\n",
        "\n",
        "Part of the code modified from: https://www.youtube.com/watch?v=zz1CFBS4NaY\n",
        "\n"
      ],
      "metadata": {
        "id": "-9mgRCBwHEVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_next_char(input_, ngram_counts, vocab, encoder, len_gram):\n",
        "\n",
        "    # Consider the last bigram of sentence\n",
        "    tokenized_input = [x.lower() for x in input_]\n",
        "    last_gram = tokenized_input[-(len_gram-1):]\n",
        "\n",
        "    # print(\"last gram\", last_gram)\n",
        "    \n",
        "    # # Calculating probability for each char in vocab\n",
        "    vocab_probabilities = {}\n",
        "\n",
        "    total = sum(ngram_counts.values())\n",
        "\n",
        "    for vocab_char in vocab:\n",
        "\n",
        "        test_gram = [last_gram[x] for x in range(len_gram-1)]\n",
        "        test_gram.append(str(vocab_char))\n",
        "        test_gram = tuple(test_gram)\n",
        "        # print(\"test_gram\", test_gram)\n",
        "        test_gram_count = ngram_counts.get(test_gram, 0)\n",
        "\n",
        "        probability = (test_gram_count / total)\n",
        "        vocab_probabilities[vocab_char] = probability\n",
        "\n",
        "        # break\n",
        "    # print(vocab_probabilities)\n",
        "    # return\n",
        "    # top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_suggestions = list(vocab_probabilities.items())\n",
        "\n",
        "    totals = sum(map(lambda x: x[1], top_suggestions))\n",
        "    coeff = 1/totals if totals != 0 else 1\n",
        "\n",
        "    top_suggestions_standardized =  [tuple([x[0], x[1] * coeff]) for x in top_suggestions]\n",
        "\n",
        "    return top_suggestions_standardized"
      ],
      "metadata": {
        "id": "V0lHDDeoJ96B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2, inf\n",
        "\n",
        "def evaluate_one(input, ngram_counts, vocab, encoder,len_gram):\n",
        "\n",
        "  input = input.lower()\n",
        "\n",
        "  max_history = 10 #max sentence length\n",
        "  history = ['<S>'] * len_gram\n",
        "  loss_anything_goes = 0\n",
        "  count = 0\n",
        "\n",
        "  for ind, c in enumerate(input):\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    loss_anything_goes -= log2(predict_next_proba(c, history, ngram_counts, vocab, encoder, len_gram))\n",
        "\n",
        "    if len(history) == max_history:\n",
        "      history.pop(0)\n",
        "\n",
        "    history.append(c)\n",
        "    \n",
        "  return [loss_anything_goes/count]\n",
        "\n",
        "\n",
        "def predict_next_proba(c, history, ngram_counts, vocab, encoder, len_gram):\n",
        "\n",
        "  # pass our x into the model and return a prediction matrix\n",
        "  y_pred = suggest_next_char(history, ngram_counts, vocab, encoder, len_gram)\n",
        "\n",
        "  # print(\"Expected \", c)\n",
        "  # print(\"top_suggestions\", y_pred, \" expected char prob: \", y_pred)\n",
        "  # print(\"\\n Char: \", c, \" Predictions: \", y_pred, \" Char prob\", y_pred[encoder[c]][1])\n",
        "\n",
        "  # return the computed probability of our character\n",
        "  proba = y_pred[encoder[c]][1] if y_pred[encoder[c]][1] != 0.0 else 0.5\n",
        "\n",
        "  return proba"
      ],
      "metadata": {
        "id": "ztiq8caHc5AP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Swahili"
      ],
      "metadata": {
        "id": "5gs9-HekIcgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest Data"
      ],
      "metadata": {
        "id": "mDO3dVLCxds0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4kqQ8iexds0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "path_train = r\"/content/sw-train.txt\"\n",
        "path_test = r\"/content/sw-test.txt\"\n",
        "\n",
        "corpus = open(path_train).readlines()[0]\n",
        "\n",
        "corpus = corpus.replace(\"\\n\", \" \")\n",
        "\n",
        "lower_case_corpus = [w.lower() for w in corpus]\n",
        "\n",
        "len_gram = 6\n",
        "\n",
        "for x in range(len_gram):\n",
        "  lower_case_corpus.insert(0, '<S>')\n",
        "\n",
        "\n",
        "vocab = sorted(list(set(lower_case_corpus)))\n",
        "# vocab.append('<S>')\n",
        "encoder = dict((c,i) for i,c in enumerate(vocab))\n",
        "decoder = dict((i,c) for i,c in enumerate(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_counts = {}\n",
        "\n",
        "# Sliding through corpus to get bigram and trigram counts\n",
        "for i in range(len(lower_case_corpus) - (len_gram-1)):\n",
        "    # Getting ngram\n",
        "\n",
        "    end = i + len_gram\n",
        "    \n",
        "    ngram = tuple([lower_case_corpus[x] for x in range(i, end)])\n",
        "    \n",
        "    # Keeping track of the bigram counts\n",
        "    if ngram in ngram_counts.keys():\n",
        "        ngram_counts[ngram] += 1\n",
        "    else:\n",
        "        ngram_counts[ngram] = 1\n",
        "    "
      ],
      "metadata": {
        "id": "MWzd0qxXPx62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate against test data"
      ],
      "metadata": {
        "id": "IIekWiGzDh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = open(path_test).readlines()\n",
        "print(\"Swahili cross entropy loss: \", evaluate_one(input[0], ngram_counts, vocab, encoder, len_gram))"
      ],
      "metadata": {
        "id": "eD8x7b7HdSpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Kwere"
      ],
      "metadata": {
        "id": "admF27klIk5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest Data"
      ],
      "metadata": {
        "id": "itGxBhQLIk5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bLgI_pGPIk5P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "path_train_ = r\"/content/cwe-train.txt\"\n",
        "path_test_ = r\"/content/cwe-test.txt\"\n",
        "\n",
        "corpus_ = open(path_train_).readlines()[0]\n",
        "\n",
        "corpus_ = corpus_.replace(\"\\n\", \" \")\n",
        "\n",
        "lower_case_corpus_ = [w.lower() for w in corpus_]\n",
        "\n",
        "len_gram_ = 6\n",
        "\n",
        "for x in range(len_gram_):\n",
        "  lower_case_corpus_.insert(0, '<S>')\n",
        "\n",
        "\n",
        "vocab_ = sorted(list(set(lower_case_corpus_)))\n",
        "encoder_ = dict((c,i) for i,c in enumerate(vocab_))\n",
        "decoder_ = dict((i,c) for i,c in enumerate(vocab_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_counts_ = {}\n",
        "\n",
        "# Sliding through corpus to get ngram counts\n",
        "for i in range(len(lower_case_corpus_) - (len_gram_-1)):\n",
        "\n",
        "    # Getting ngram\n",
        "\n",
        "    end = i + len_gram_\n",
        "    \n",
        "    ngram = tuple([lower_case_corpus_[x] for x in range(i, end)])\n",
        "    \n",
        "    # Keeping track of the ngram counts\n",
        "    if ngram in ngram_counts_.keys():\n",
        "        ngram_counts_[ngram] += 1\n",
        "    else:\n",
        "        ngram_counts_[ngram] = 1\n",
        "    "
      ],
      "metadata": {
        "id": "44b6tuYYIk5P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate against test data"
      ],
      "metadata": {
        "id": "menlAHbuIk5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = open(path_test_).readlines()\n",
        "input_[0] = input_[0].replace(\"\\n\", \" \")\n",
        "print(\"Kwere cross entropy loss: \", evaluate_one(input_[0], ngram_counts_, vocab_, encoder_, len_gram_))"
      ],
      "metadata": {
        "id": "lF9tYyjNIk5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fa8b73-beee-4317-a006-b48ec0a98f76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kwere cross entropy loss:  [1.2741946778164173]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate sentences"
      ],
      "metadata": {
        "id": "1PmWZY7XDvGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = 'maji katika'\n",
        "sent_len = 100\n",
        "\n",
        "for i in range(sent_len):\n",
        "  next_char = suggest_next_char(input_sent, ngram_counts_, vocab, encoder)\n",
        "  print(input_sent)\n",
        "  input_sent = input_sent + next_char[0][0]\n",
        "\n",
        "print(input_sent)\n",
        "\n"
      ],
      "metadata": {
        "id": "NTBNGsgS9VtF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}