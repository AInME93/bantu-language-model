{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## N-Gram\n",
        "\n",
        "For this solution we use the N-Gram model approach. The steps will be as follows:\n",
        "\n",
        "1. Ingest data\n",
        "2. Generate vocab, encoder, decoder\n",
        "3. Generate a count dictionary of all n-grams generated from our data. This will serve as our frequency lookup table.\n",
        "4. Read test data.\n",
        "5. Iteratively for each character:\n",
        "\n",
        "  * generate a bigram matching the length specified earlier. Where necessary, using padding character to maintain length.\n",
        "\n",
        "  * Use our frequency lookup table to return count of the current bigram \n",
        "\n",
        "  * Divide by total no of occurences of all bigrams \n",
        "\n",
        "  * Normalize these probabilities and return the probability matrix generated\n",
        "\n",
        "  * Look up generated probability of our expected character\n",
        "\n",
        "  * calculate log of the value and subtract from overall loss of the entire model\n",
        "\n",
        "  * remove first character of our bigram and append current character to the end of the bigram\n",
        "\n",
        "\n",
        "Part of the code modified from: https://www.youtube.com/watch?v=zz1CFBS4NaY\n",
        "\n"
      ],
      "metadata": {
        "id": "-9mgRCBwHEVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_next_char(input_, ngram_counts, vocab_counts, vocab, encoder, len_gram, laplace_weight=0):\n",
        "\n",
        "    # Consider the last bigram of sentence\n",
        "    tokenized_input = [x.lower() for x in input_]\n",
        "    last_gram = tokenized_input[-(len_gram):]\n",
        "\n",
        "    # print(\"last gram\", last_gram)\n",
        "    \n",
        "    # # Calculating probability for each char in vocab\n",
        "    vocab_probabilities = {}\n",
        "\n",
        "    total_ngrams = sum(ngram_counts.values())\n",
        "    total_vocab = sum(vocab_counts.values())\n",
        "\n",
        "    for vocab_char in vocab:\n",
        "\n",
        "        original_gram = test_gram = [last_gram[x] for x in range(len_gram)]\n",
        "        test_gram.pop()\n",
        "        test_gram.insert(0,str(vocab_char))\n",
        "        likelihood_gram = tuple(test_gram)\n",
        "        likelihood_gram_count = ngram_counts.get(likelihood_gram, 0)\n",
        "\n",
        "\n",
        "        test_gram.pop(0)\n",
        "        test_gram.append(str(vocab_char))\n",
        "        test_gram = tuple(test_gram)\n",
        "        test_gram_count = ngram_counts.get(test_gram, 0)\n",
        "\n",
        "        # Naive Bayes probability\n",
        "        # posterior = (likelihood * prior)/evidence\n",
        "        likelihood = likelihood_gram_count / total_ngrams\n",
        "        prior = vocab_counts[vocab_char] / total_vocab\n",
        "        evidence = ngram_counts.get(tuple(original_gram), 0) / total_ngrams\n",
        "\n",
        "        probability = ((likelihood * prior) + laplace_weight)/ (evidence + laplace_weight)\n",
        "\n",
        "        vocab_probabilities[vocab_char] = probability\n",
        "\n",
        "    # top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_suggestions = list(vocab_probabilities.items())\n",
        "\n",
        "    totals = sum(map(lambda x: x[1], top_suggestions))\n",
        "    coeff = 1/totals if totals != 0 else 1\n",
        "\n",
        "    top_suggestions_standardized =  [tuple([x[0], x[1] * coeff]) for x in top_suggestions]\n",
        "\n",
        "    return top_suggestions_standardized"
      ],
      "metadata": {
        "id": "V0lHDDeoJ96B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2, inf\n",
        "\n",
        "def evaluate_one(input, ngram_counts, vocab_counts, vocab, encoder,len_gram, laplace_weight=0):\n",
        "\n",
        "  input = input.lower()\n",
        "\n",
        "  max_history = 10 #max sentence length\n",
        "  history = ['<S>'] * len_gram\n",
        "  loss_anything_goes = 0\n",
        "  count = 0\n",
        "\n",
        "  for ind, c in enumerate(input):\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    loss_anything_goes -= log2(predict_next_proba(c, history, ngram_counts, vocab_counts, vocab, encoder, len_gram, laplace_weight))\n",
        "\n",
        "    if len(history) == max_history:\n",
        "      history.pop(0)\n",
        "\n",
        "    history.append(c)\n",
        "    \n",
        "  return [loss_anything_goes/count]\n",
        "\n",
        "\n",
        "def predict_next_proba(c, history, ngram_counts, vocab_counts, vocab, encoder, len_gram, laplace_weight=0):\n",
        "\n",
        "  # pass our x into the model and return a prediction matrix\n",
        "  y_pred = suggest_next_char(history, ngram_counts, vocab_counts, vocab, encoder, len_gram, laplace_weight)\n",
        "\n",
        "  # print(\"Expected \", c)\n",
        "  # print(\"top_suggestions\", y_pred, \" expected char prob: \", y_pred)\n",
        "  # print(\"\\n Char: \", c, \" Predictions: \", y_pred, \" Char prob\", y_pred[encoder[c]][1])\n",
        "\n",
        "  # return the computed probability of our character\n",
        "  proba = y_pred[encoder[c]][1]\n",
        "\n",
        "  return proba"
      ],
      "metadata": {
        "id": "ztiq8caHc5AP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Swahili"
      ],
      "metadata": {
        "id": "5gs9-HekIcgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest Data"
      ],
      "metadata": {
        "id": "mDO3dVLCxds0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i4kqQ8iexds0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "path_train = r\"/content/sw-train.txt\"\n",
        "path_test = r\"/content/sw-test.txt\"\n",
        "\n",
        "corpus = open(path_train).readlines()[0]\n",
        "\n",
        "corpus = corpus.replace(\"\\n\", \" \")\n",
        "\n",
        "lower_case_corpus = [w.lower() for w in corpus]\n",
        "\n",
        "len_gram = 4\n",
        "\n",
        "for x in range(len_gram):\n",
        "  lower_case_corpus.insert(0, '<S>')\n",
        "\n",
        "\n",
        "vocab = sorted(list(set(lower_case_corpus)))\n",
        "# vocab.append('<S>')\n",
        "encoder = dict((c,i) for i,c in enumerate(vocab))\n",
        "decoder = dict((i,c) for i,c in enumerate(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_counts = {}\n",
        "vocab_counts = {}\n",
        "vocab_counts['<S>'] = len_gram - 1\n",
        "\n",
        "# Sliding through corpus to get ngram counts\n",
        "for i in range(len(lower_case_corpus) - (len_gram-1)):\n",
        "\n",
        "    # Getting ngram\n",
        "    end = i + len_gram\n",
        "    new_char = lower_case_corpus[end-1]\n",
        "\n",
        "    # Track vocab counts\n",
        "    if new_char in vocab_counts.keys():\n",
        "      vocab_counts[new_char] += 1\n",
        "    else:\n",
        "      vocab_counts[new_char] = 1\n",
        "\n",
        "    ngram = tuple([lower_case_corpus[x] for x in range(i, end)])\n",
        "\n",
        "    # Keeping track of the ngram counts\n",
        "    if ngram in ngram_counts.keys():\n",
        "        ngram_counts[ngram] += 1\n",
        "    else:\n",
        "        ngram_counts[ngram] = 1\n",
        "    "
      ],
      "metadata": {
        "id": "MWzd0qxXPx62"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate against test data"
      ],
      "metadata": {
        "id": "IIekWiGzDh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = open(path_test).readlines()\n",
        "print(\"Swahili cross entropy loss: \", evaluate_one(input[0][0:50000], ngram_counts, vocab_counts, vocab, encoder, len_gram, laplace_weight=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iy6vXCUnkBa",
        "outputId": "450e6d8c-2649-424e-ebaf-f0afc06ca8ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swahili cross entropy loss:  [5.614758135501889]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Kwere"
      ],
      "metadata": {
        "id": "admF27klIk5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest Data"
      ],
      "metadata": {
        "id": "itGxBhQLIk5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bLgI_pGPIk5P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "path_train_ = r\"/content/cwe-train.txt\"\n",
        "path_test_ = r\"/content/cwe-test.txt\"\n",
        "\n",
        "corpus_ = open(path_train_).readlines()[0]\n",
        "\n",
        "corpus_ = corpus_.replace(\"\\n\", \" \")\n",
        "\n",
        "lower_case_corpus_ = [w.lower() for w in corpus_]\n",
        "\n",
        "len_gram_ = 4\n",
        "\n",
        "for x in range(len_gram_):\n",
        "  lower_case_corpus_.insert(0, '<S>')\n",
        "\n",
        "\n",
        "vocab_ = sorted(list(set(lower_case_corpus_)))\n",
        "encoder_ = dict((c,i) for i,c in enumerate(vocab_))\n",
        "decoder_ = dict((i,c) for i,c in enumerate(vocab_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_counts_ = {}\n",
        "vocab_counts_ = {}\n",
        "vocab_counts_['<S>'] = len_gram_-1\n",
        "\n",
        "# Sliding through corpus to get ngram counts\n",
        "for i in range(len(lower_case_corpus_) - (len_gram_-1)):\n",
        "\n",
        "    # Getting ngram\n",
        "    end = i + len_gram_\n",
        "    new_char = lower_case_corpus_[end-1]\n",
        "\n",
        "    # Track vocab counts\n",
        "    if new_char in vocab_counts_.keys():\n",
        "      vocab_counts_[new_char] += 1\n",
        "    else:\n",
        "      vocab_counts_[new_char] = 1\n",
        "      \n",
        "\n",
        "    ngram = tuple([lower_case_corpus_[x] for x in range(i, end)])\n",
        "    # print(end,ngram)\n",
        "    # Keeping track of the ngram counts\n",
        "    if ngram in ngram_counts_.keys():\n",
        "        ngram_counts_[ngram] += 1\n",
        "    else:\n",
        "        ngram_counts_[ngram] = 1\n",
        "    "
      ],
      "metadata": {
        "id": "44b6tuYYIk5P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate against test data"
      ],
      "metadata": {
        "id": "menlAHbuIk5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = open(path_test_).readlines()\n",
        "input_[0] = input_[0].replace(\"\\n\", \" \")\n",
        "print(\"Kwere cross entropy loss: \", evaluate_one(input_[0], ngram_counts_, vocab_counts_, vocab_, encoder_,len_gram = len_gram_,laplace_weight = 1))"
      ],
      "metadata": {
        "id": "lF9tYyjNIk5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7967fb2a-0729-451f-b18e-2c1c7c9c079a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kwere cross entropy loss:  [4.999983825016389]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate sentences"
      ],
      "metadata": {
        "id": "1PmWZY7XDvGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = 'maji katika'\n",
        "sent_len = 100\n",
        "\n",
        "for i in range(sent_len):\n",
        "  next_char = suggest_next_char(input_sent, ngram_counts_, vocab, encoder)\n",
        "  print(input_sent)\n",
        "  input_sent = input_sent + next_char[0][0]\n",
        "\n",
        "print(input_sent)\n",
        "\n"
      ],
      "metadata": {
        "id": "NTBNGsgS9VtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [0, 1, 2]"
      ],
      "metadata": {
        "id": "SrO1HGQ7weZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.insert(0, 3)"
      ],
      "metadata": {
        "id": "RLqLUh6iwhRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.pop(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKCBJRzgwkae",
        "outputId": "262e6b2f-c255-474c-d334-15365ddd0d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljbEAAt_wmml",
        "outputId": "f79abe0b-376a-4c49-9d94-1aafc19896dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NqTdGPfbwtHL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}