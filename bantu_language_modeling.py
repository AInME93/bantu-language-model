# -*- coding: utf-8 -*-
"""Bantu_Language_Modeling (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hrUmTXSehuclX8tiKQsFPxe0gZKftyts

# Bantu Language Modeling

## Data Loading and preprocessing
"""

import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras.utils import to_categorical

class Preprocessing:

    def __init__(self, data:list = None) -> None:
        self.data = data
        self.vocab = None
        self.encoder = None
        self.decoder = None
    
                
    # def remove_punctuation(self):

    #     import string

    #     punctuation_list = [x for x in string.punctuation]
    #     exclude_list = ["'", "-", "!", "."]
    #     punctuation_list = list(set(punctuation_list) - set(exclude_list))

    #     for ind,word in enumerate(self.data):  
    #       for punctuation in punctuation_list:
    #         if punctuation in word:
    #           word = word.replace(punctuation, "")

    #       self.data[ind] = word
    
    # def set_train(self, path = None):
    #   pass

    def read_file(self, path):

      '''
      https://colab.research.google.com/drive/1pFL6pHFsG6QAl0th99mIKsPaUJ84w6gN?usp=sharing#scrollTo=XM3TiINWVhyf      
      '''

      self.data= open(path).readlines()

      # data_raw = tf.data.TextLineDataset([path])

      # for elem in data_raw:
      #   self.data=self.data+(elem.numpy().decode('utf-8'))

      # self.data = text.split(" ")
      # self.data = [a.strip() for a in self.data]

      # return self.data, self.vocab

    def set_vocab_encoder_decoder(self):
      '''
      https://colab.research.google.com/drive/16_OYcypTu0OtA1eOT6Zjg65atzXFX9GH#scrollTo=DCf7rn8bX3Ws      
      '''

      self.vocab = set([c for a in self.data[0] for c in a])
      self.vocab.add('<PAD>')
      self.encoder = dict((c,i) for i,c in enumerate(self.vocab))
      self.decoder = dict((i,c) for i,c in enumerate(self.vocab))

    def set_X_y(self, sample_size:list = None):
      
      print("Characters len: ", len(self.data[0]))

      if sample_size is not None:
        if len(sample_size) == 1:
          self.data = self.data[0][0:sample_size[0]]
        else:
          self.data = self.data[0][sample_size[0]:sample_size[1]]

      X = []
      y = []

      inputlen = 10

      for a in self.data:
        Xenc = [self.encoder['<PAD>']]*inputlen

        for c in a:
            X.append(Xenc.copy())
            y.append(self.encoder[c])
            Xenc.pop(0)
            Xenc.append(self.encoder[c])

        X.append(Xenc.copy())
        y.append(self.encoder['<PAD>'])
        
      X = np.array(X)
      y = to_categorical(y, num_classes=len(self.vocab))

      return X, y

"""## Model Definitions"""

import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout, LSTM
from tensorflow.keras import Model,Input
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import save_model, load_model
import time

class Model:
  
  def __init__(self, X_train, X_test, y_train, y_test, input_dim, output_dim, input_length):
    self.model = None
    self.X_train, self.X_test, self.y_train, self.y_test = X_train, X_test, y_train, y_test
    self.y_pred = None
    self.input_dim, self.output_dim, self.input_length = input_dim, output_dim, input_length
    self.batch_size = 64
    self.epochs = 50
    self.history = None
    self.model_path = None

  def build_model_variant1(self):

    # tensorflow.keras.Input(shape=(maxlen, len(characters))),
    # tensorflow.keras.layers.LSTM(128,return_sequences=True),
    # tensorflow.keras.layers.Dropout(0.3),
    # tensorflow.keras.layers.LSTM(128,return_sequences=True),
    # tensorflow.keras.layers.Dropout(0.3),
    # tensorflow.keras.layers.LSTM(128),
    # tensorflow.keras.layers.Dense(len(characters), activation="softmax")

    self.model = Sequential()
    self.model.add(Embedding(input_dim=self.input_dim, output_dim=self.output_dim,\
                             input_length=self.input_length))
    self.model.add(Flatten())
    self.model.add(LSTM(128,return_sequences=False)),
    self.model.add(Dropout(0.3)),
    self.model.add(LSTM(128,return_sequences=False)),
    self.model.add(Dropout(0.3)),
    self.model.add(LSTM(128,return_sequences=False)),
    self.model.add(Dense(self.input_dim, activation='softmax'))
    optimizer = Adam(learning_rate=1e-3, decay=1e-5)
    self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)
    self.model.summary() 

  def build_model_variant2(self):

    self.model = Sequential()
    self.model.add(Embedding(input_dim=self.input_dim, output_dim=self.output_dim,\
                             input_length=self.input_length))
    self.model.add(Dropout(0.5))
    self.model.add(LSTM(128, return_sequences=False))
    self.model.add(Dense(self.input_dim, activation='softmax'))

    optimizer = Adam(learning_rate=1e-3, decay=1e-5)
    self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)
    self.model.summary() 

  def model_fit(self, batch_size=None, epochs = None, name=""):

    if batch_size is not None:
      self.batch_size = batch_size

    if epochs is not None:
      self.epochs = epochs

    # save model history locally
    # https://stackoverflow.com/questions/47843265/how-can-i-get-a-keras-models-history-after-loading-it-from-a-file-in-python
    from keras.callbacks import CSVLogger
    tagger = str(time.time())[0:10]
    csv_logger = CSVLogger(name + "_" + tagger + '_training.log', separator=',', append=False)

    self.history = self.model.fit(self.X_train, self.y_train, \
                                  batch_size=self.batch_size, \
                                  epochs=self.epochs, \
                                  validation_split=0.15, \
                                  callbacks=[csv_logger],
                                  verbose=2)

    # save model
    path = os.getcwd() + "/" + name + "_" + tagger + "_" +  "loss_" + \
          str(self.history.history["val_loss"][-1])[0:6].replace(".", "_") + \
          "_model.h5"
    self.model_path = path
    save_model(self.model, path)


  def model_predict(self, X_test = None, y_test = None):
    if X_test is not None:
      self.X_test = X_test

    if y_test is not None:
      self.y_test = y_test

    self.y_pred = self.model.predict(self.X_test)

    for y in self.y_pred:
      for ind, val in enumerate(y):
        if val == max(y):
          y[ind] = 1
        else:
          y[ind] = 0


  def model_evaluate(self):

    for y in self.y_pred:
      for ind, val in enumerate(y):
        if val == max(y):
          y[ind] = 1
        else:
          y[ind] = 0
    # [0,0,0,1]
    # [0,0,1,0]
    [1e-2332,2e-2323]
    return accuracy_score(self.y_test, self.y_pred)


  def load_model(self, path):
    self.model = load_model(path)

"""## Evaluate"""

path_sw = r"/content/sw-train.txt"
path_kw = r"/content/cwe-train.txt"

from sklearn.model_selection import train_test_split

def make_model(path,sample_size=600000, name=None):

  prep = Preprocessing()
  prep.read_file(path)
  prep.set_vocab_encoder_decoder()

  print('Data ingested')

  X, y = prep.set_X_y(sample_size = sample_size)

  print('X, y set')

  input_dim = len(prep.vocab)
  inputlen = emb_dim = 10

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)
  k_m = Model(X_train, X_test, y_train, y_test , input_dim, emb_dim, inputlen)

  k_m.build_model_variant2()
  km_hist = k_m.model_fit(batch_size = 1024, epochs = 10, name=name)
  k_m.model_predict()
  k_m.model_evaluate()

def anything_goes():
  km = make_model(path_kw, name='kw')
  sm = make_model(path_sw, sample_size=4000000,name='sw')

"""### Kwere"""

# KWERE

k_p = Preprocessing()
k_p.read_file(path_kw)
k_p.set_vocab_encoder_decoder()

print('Data ingested')

kw_X, kw_y = k_p.set_X_y(sample_size = [600000])

print('X, y set')

input_dim = len(k_p.vocab)
inputlen = emb_dim = 10

kw_X_train, kw_X_test, kw_y_train, kw_y_test = train_test_split(kw_X, kw_y, test_size=0.2, random_state = 42)
k_m = Model(kw_X_train, kw_X_test, kw_y_train, kw_y_test, input_dim, emb_dim, inputlen)

k_m.build_model_variant2()
km_hist = k_m.model_fit(batch_size = 512, epochs = 50, name="kw")
k_m.model_predict()
k_m.model_evaluate()


# sm = make_model(path_sw)
# km = make_model(path_kw)

k_m.model_predict()
k_m.model_evaluate()

path_test_kw = "/content/cwe-test.txt"
path_model = ""

k_p.read_file(path_test_kw)
kw_X, kw_y = k_p.set_X_y(sample_size = 600000)

k_m.load_model(path_model)
k_m.model_predict(kw_X, kw_y)
print("Accuracy: ", k_m.model_evaluate())

"""### Swahili"""

from time import sleep

# SWAHILI
s_p = Preprocessing()
s_p.read_file(path_sw)
s_p.set_vocab_encoder_decoder()

counter = 0
data_len = len(s_p.data[0])

# Train in stages to prevent session crashing for a total of 50 epochs
while counter < 40:
  sleep(60)
  start = int((data_len / 40) * counter)
  end = int(((data_len / 40) * (counter+1)) - 1)

  print(start, end)

  s_p.read_file(path_sw)

  # print('Data ingested')

  sw_X, sw_y = s_p.set_X_y(sample_size = [start, end])

  # print('X, y set')

  input_dim = len(s_p.vocab)
  inputlen = emb_dim = 10

  sw_X_train, sw_X_test, sw_y_train, sw_y_test = train_test_split(sw_X, sw_y, test_size=0.2, random_state = 42)

  if counter == 0:
    s_m = Model(sw_X_train, sw_X_test, sw_y_train, sw_y_test, input_dim, emb_dim, inputlen)
    s_m.build_model_variant2()

  else:
    s_m.load_model(model_path) 

  counter += 1

  s_m.model_fit(batch_size = 512, epochs = 1, name="sw")
  model_path = s_m.model_path
  print(model_path)



# # sm = make_model(path_sw)
# # km = make_model(path_kw)

s_m.model_predict()
print("Accuracy: " , s_m.model_evaluate())

path_kw = r"/content/cwe-train.txt"

k_p = Preprocessing()
k_p.read_file(path_kw)
k_p.set_vocab_encoder_decoder()

kw_X, kw_y = k_p.set_X_y()
kw_X_train, kw_X_test, kw_y_train, kw_y_test = train_test_split(kw_X, kw_y, test_size=0.2, random_state = 42)

input_dim = len(k_p.vocab)
inputlen = emb_dim = 10

k_m = Model(kw_X_train, kw_X_test, kw_y_train, kw_y_test, input_dim, emb_dim, inputlen)

# k_m.build_model_variant2()
# k_m.model_fit(batch_size = 512, epochs = 2, name="w")

"""## Cross Entropy evaluation"""

def sample(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.max(probas)

from math import log2

def evaluate_one(lang, model, vocab, encoder):

  testfile = open(lang+'-test.txt', 'r')
  max_history = 100
  history = []
  loss_anything_goes = 0
  count = 0

  while True:

    c = testfile.read(1)
    if not c:
      break
    count += 1
    loss_anything_goes -= log2(predict_next_proba( c, model, vocab, encoder))
    if len(history) == max_history:
      history.pop(0)
    history.append(c)

  return [loss_anything_goes/count]


def predict_next_proba(c, model, vocab, encoder):

  inputlen = 10
  X = []
  Xenc = [encoder['<PAD>']]*inputlen
  Xenc.pop(0)
  Xenc.append(encoder[c])

  X.append(Xenc)
  X = np.array(X)
    
  y_pred = model.predict(np.array(X),verbose = 0)
  # print(round(sum(y_pred[0]), 15))

  proba = max(y_pred[0])
  # print(proba)

  # print(sum(y_pred[0]))

  return proba

swahili_loss = evaluate_one("sw", s_m.model,s_p.vocab, s_p.encoder)

print("Swahili Cross Entropy Loss: ", swahili_loss)

kwere_loss = evaluate_one("cwe", k_m.model,k_p.vocab, k_p.encoder)

print("Kwere Cross Entropy Loss: ", kwere_loss)